{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42600d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import lightning.pytorch as L\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from fg_funcs import vae_loss, visualize_latent_space, calculate_reconstruction_quality, get_nearest_neighbors, metric, get_fg_counts, average_latent_vector, plot_average_latent_vectors, plot_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45f534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sampled data into a pandas dataframe\n",
    "sampled = pd.read_pickle('chembl_35_fg_scaf_curated.pkl')\n",
    "\n",
    "# Convert the fingerprint to numpy arrays\n",
    "sampled['fingerprint_array'] = sampled['fingerprint_array'].apply(lambda x: x if isinstance(x, np.ndarray) else np.zeros((2048,), dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501a0479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a flexible VAE model with customizable encoder/decoder hidden layers\n",
    "class SimpleVAE(L.LightningModule):\n",
    "    def __init__(self, input_dim, latent_dim, encoder_hidden_dims, decoder_hidden_dims):\n",
    "        super(SimpleVAE, self).__init__()\n",
    "        # Build encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in encoder_hidden_dims:\n",
    "            encoder_layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            encoder_layers.append(nn.ReLU())\n",
    "            prev_dim = h_dim\n",
    "        encoder_layers.append(nn.Linear(prev_dim, latent_dim * 2))  # Output mu and log_var\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Build decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for h_dim in decoder_hidden_dims:\n",
    "            decoder_layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            decoder_layers.append(nn.ReLU())\n",
    "            prev_dim = h_dim\n",
    "        decoder_layers.append(nn.Linear(prev_dim, input_dim))\n",
    "        decoder_layers.append(nn.Sigmoid())  # Assuming input is normalized between 0 and 1\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu, log_var = h.chunk(2, dim=-1)\n",
    "        return mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decode(z), mu, log_var\n",
    "    \n",
    "# Create data loader\n",
    "class FingerprintDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, fingerprint_col='fingerprint_array', fg_col='fg_array'):\n",
    "        self.data = data\n",
    "        self.fingerprint_col = fingerprint_col\n",
    "        self.fg_col = fg_col\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        fingerprint = torch.tensor(row[self.fingerprint_col], dtype=torch.float32)\n",
    "        fg_vector = torch.tensor(row[self.fg_col], dtype=torch.float32)\n",
    "        return fingerprint, fg_vector, idx\n",
    "    \n",
    "class SimpleVAETrainer(L.LightningModule):\n",
    "    def __init__(self, input_dim, encoder_hidden_dim, decoder_hidden_dim, latent_dim, learning_rate):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = SimpleVAE(\n",
    "            input_dim=input_dim,\n",
    "            latent_dim=latent_dim,\n",
    "            encoder_hidden_dims=encoder_hidden_dim,\n",
    "            decoder_hidden_dims=decoder_hidden_dim\n",
    "        )\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _, _ = batch\n",
    "        recon_x, mu, log_var = self(x)\n",
    "        beta = min(1.5, self.current_epoch / 5)  # Gradually increase beta\n",
    "        loss, bce, kld = vae_loss(recon_x, x, mu, log_var, beta)\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_bce', bce)\n",
    "        self.log('train_kld', kld)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, _, _ = batch\n",
    "        recon_x, mu, log_var = self(x)\n",
    "        loss, bce, kld = vae_loss(recon_x, x, mu, log_var)\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_bce', bce, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_kld', kld, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, _, _ = batch\n",
    "        recon_x, mu, log_var = self(x)\n",
    "        loss, bce, kld = vae_loss(recon_x, x, mu, log_var)\n",
    "        self.log('test_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_bce', bce, on_epoch=True, prog_bar=True)\n",
    "        self.log('test_kld', kld, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcd87f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "input_dim = 2048  # Size of the fingerprint\n",
    "encoder_hidden_dim = [1024, 512, 256, 128, 64]  # Size of the hidden layer\n",
    "decoder_hidden_dim = [256, 1024]  # Size of the hidden layer for decoder\n",
    "latent_dim = 8  # Size of the latent space\n",
    "batch_size = 64  # Batch size for training\n",
    "num_epochs = 5  # Number of epochs for training\n",
    "learning_rate = 0.001  # Learning rate for the optimizer\n",
    "\n",
    "# Initialize wandb for logging\n",
    "wandb_logger = WandbLogger(project='simple_vae_fingerprint', log_model=True)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create training and testing dataset and data loader\n",
    "train_data, test_data = train_test_split(sampled, test_size=0.2, random_state=42)\n",
    "train_split, val_split = train_test_split(train_data, test_size=0.1, random_state=42)\n",
    "train_dataset = FingerprintDataset(train_split)\n",
    "val_dataset = FingerprintDataset(val_split)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataset = FingerprintDataset(test_data)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = L.Trainer(max_epochs=num_epochs, val_check_interval=0.5, logger=wandb_logger)\n",
    "vae_trainer = SimpleVAETrainer(input_dim, encoder_hidden_dim, decoder_hidden_dim, latent_dim, learning_rate)\n",
    "trainer.fit(vae_trainer, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "# Evaluate the model\n",
    "trainer.test(vae_trainer, test_loader)\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf0571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_latent_space(vae_trainer.model, train_dataset, fg_indicies=[0,1,2,3], method='tsne', sample_size=10000)\n",
    "visualize_latent_space(vae_trainer.model, train_dataset, fg_indicies=[0,1,2,3], method='pca', sample_size=10000)\n",
    "visualize_latent_space(vae_trainer.model, train_dataset, fg_indicies=[0,1,2,3], method='umap', sample_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ffb3b1",
   "metadata": {},
   "source": [
    "### TSNE/UMAP on morgan fingerprints\n",
    "### Distances between fg groupings in latent space see if it aligns with current knowledge\n",
    "### More investigation into the scaffolds\n",
    "\n",
    "### Clustering algorithm on latent check its quality against fgs\n",
    "### Run the pKa molecules through vae and check correlation\n",
    "### Scan latent space with some kernel on small regions (maybe avg tanimoto similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f78c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# umap_embeddings = perform_umap_on_fingerprints(sampled, fingerprint_col='fingerprint_array', fg_col='fg_array', n_neighbors=15, min_dist=0.1, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea59bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tanimoto_scores, f1_scores, precision_scores, recall_scores, uniqueness, mcc_scores = calculate_reconstruction_quality(vae_trainer.model, test_dataset, threshold=0.5)\n",
    "print(\"Tanimoto Scores:\", np.mean(tanimoto_scores))\n",
    "print(\"F1 Scores:\", np.mean(f1_scores)) \n",
    "print(\"Precision:\", np.mean(precision_scores))\n",
    "print(\"Recall:\", np.mean(recall_scores))\n",
    "print(\"Uniqueness of reconstructions:\", uniqueness)\n",
    "print(\"Matthews Correlation Coefficient:\", np.mean(mcc_scores))\n",
    "\n",
    "plots = [\n",
    "    {\n",
    "        'data': tanimoto_scores,\n",
    "        'title': 'Tanimoto Scores Distribution',\n",
    "        'xlabel': 'Tanimoto Score',\n",
    "    },\n",
    "    {\n",
    "        'data': f1_scores,\n",
    "        'title': 'F1 Scores Distribution',\n",
    "        'xlabel': 'F1 Score',\n",
    "    },\n",
    "    {\n",
    "        'data': precision_scores,\n",
    "        'title': 'Precision Scores Distribution',\n",
    "        'xlabel': 'Precision Score',\n",
    "    },\n",
    "    {\n",
    "        'data': recall_scores,\n",
    "        'title': 'Recall Scores Distribution',\n",
    "        'xlabel': 'Recall Score',\n",
    "    },\n",
    "    {\n",
    "        'data': mcc_scores,\n",
    "        'title': 'Matthews Correlation Coefficient Distribution',\n",
    "        'xlabel': 'MCC Score',\n",
    "    }\n",
    "]\n",
    "\n",
    "plot_distributions(plots)\n",
    "\n",
    "avg_latents = average_latent_vector(vae_trainer.model, train_dataset, fg_col='fg_array', fg_indicies=[0, 1, 2, 3])\n",
    "plot_average_latent_vectors(avg_latents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d102c49",
   "metadata": {},
   "source": [
    "# Organization Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299bb190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from fg_funcs import find_nearest_neighbors, weighted_tanimoto, Dataset\n",
    "def temp_metric(\n",
    "    nbrs: NearestNeighbors,\n",
    "    dataset: Dataset,\n",
    "    query_index: int,\n",
    "    latent: np.ndarray,\n",
    "    fg_counts: np.ndarray,\n",
    "    fg_col: str = 'fg_array',\n",
    "    n_neighbors: int = 50\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Combined metric: Weighted Tanimoto similarity normalized by expected random similarity.\n",
    "    \"\"\"\n",
    "    query_fg = dataset.data.iloc[query_index][fg_col]  # np.ndarray\n",
    "    distances, indices = find_nearest_neighbors(nbrs, latent[query_index], n_neighbors)\n",
    "\n",
    "    # Inverse prevalence weights (avoid division by zero)\n",
    "    weights = 1.0 / (fg_counts + 1e-8)\n",
    "    weights /= np.sum(weights)  # Normalize weights\n",
    "\n",
    "    tanimoto_scores = []\n",
    "    for idx in indices:\n",
    "        neighbor_fg = dataset.data.iloc[idx][fg_col]\n",
    "        tanimoto_scores.append(weighted_tanimoto(query_fg, neighbor_fg, weights))\n",
    "\n",
    "    avg_weighted_tanimoto = np.mean(tanimoto_scores) if len(tanimoto_scores) > 0 else 0.0\n",
    "\n",
    "    # # Scale tanimoto scores by the normalized distance from the query point\n",
    "    # normalized_distances = distances / np.max(distances) if np.max(distances) > 0 else distances\n",
    "    # scaled_tanimoto_scores = np.array(tanimoto_scores) / (normalized_distances + 1e-8)\n",
    "    # avg_weighted_tanimoto = np.mean(scaled_tanimoto_scores) if len(scaled_tanimoto_scores) > 0 else 0.0\n",
    "    \n",
    "    return avg_weighted_tanimoto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c4b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get nearest neighbors for the first 10 samples in the dataset\n",
    "n_neighbors = 50\n",
    "metric_set = train_dataset\n",
    "nbrs, latent_vectors, fg_labels = get_nearest_neighbors(vae_trainer.model, metric_set, n_neighbors=n_neighbors)\n",
    "\n",
    "\n",
    "# Example usage for the first 10 samples\n",
    "fg_counts = get_fg_counts(metric_set, fg_col='fg_array')\n",
    "print(f\"Percentage of all molecules with each functional group: {fg_counts[0]* 100:.2f}% for FG0, {fg_counts[1]* 100:.2f}% for FG1, {fg_counts[2]* 100:.2f}% for FG2, {fg_counts[3]* 100:.2f}% for FG3\\n\")\n",
    "\n",
    "# Get metric scores for the first 10 samples\n",
    "scores = []\n",
    "for i in range(len(metric_set)):\n",
    "    metric_score = temp_metric(nbrs, metric_set, i, latent=latent_vectors, fg_counts=fg_counts, fg_col='fg_array', n_neighbors=n_neighbors)\n",
    "    scores.append(metric_score)\n",
    "\n",
    "scores = np.array(scores)\n",
    "\n",
    "# Group scores by functional group\n",
    "scores_df = pd.DataFrame(scores, columns=['score'])\n",
    "fg_strings = [np.array2string(fg, separator=',') for fg in fg_labels]\n",
    "scores_df['fg'] = fg_strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe927f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert fg strings into single number\n",
    "fg_mapping = {'[1.,0.,0.,0.]': 0, '[0.,1.,0.,0.]': 1, '[0.,0.,1.,0.]': 2, '[0.,0.,0.,1.]': 3}\n",
    "scores_df['fg'] = scores_df['fg'].map(fg_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e4aad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the average score for each functional group\n",
    "avg_scores = scores_df.groupby('fg')['score'].mean().reset_index()\n",
    "avg_scores.columns = ['Functional Group', 'Average Score']\n",
    "\n",
    "print(\"Average Scores by Functional Group:\")\n",
    "print(avg_scores)\n",
    "\n",
    "print(scores_df['fg'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42b823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple kNN classifier to predict functional groups\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Train on latent vectors\n",
    "X = latent_vectors\n",
    "y = scores_df['fg'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=len(scores_df['fg'].unique()))\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdkit-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
