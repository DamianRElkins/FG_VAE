{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5e35dc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import os\n",
    "import random\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, AllChem, DataStructs, QED\n",
    "from efgs import get_dec_fgs\n",
    "import io\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import re\n",
    "\n",
    "from VAE import (\n",
    "    train_conditional_vae, \n",
    "    train_conditional_subspace_vae, \n",
    "    train_discover_vae, \n",
    "    train_base_model,\n",
    "    FingerprintDataset,\n",
    "    BaseVAETrainer,\n",
    "    ConditionalVAETrainer,\n",
    "    ConditionalSubspaceVAETrainer,\n",
    "    DiscoverVAETrainer,\n",
    "    )\n",
    "from fg_funcs import (\n",
    "    extract_and_save_latents,\n",
    "    extract_prefixed_arrays,\n",
    "    evaluate_reconstructions,\n",
    "    visualize_latent_space_per_fg,\n",
    "    metric,\n",
    "    get_nearest_neighbors,\n",
    "    visualize_latent_space_tanimoto,\n",
    "    fingerprint_to_bv,\n",
    "    mol_to_fingerprint\n",
    ")\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from rdkit.DataStructs.cDataStructs import TanimotoSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94b60645",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = []\n",
    "GET_LATENTS = True\n",
    "GET_EVAL = True\n",
    "VIS_CUR = True\n",
    "VIS_FULL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c72ccbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dims = [4, 8, 16]  # You can change this to test different latent dimensions\n",
    "encoder_hidden_dims = [1024, 512, 256, 128]\n",
    "decoder_hidden_dims = [128, 512]  \n",
    "decoder_z_hidden_dims = [128, 512]  # Decoder layers for DISCoVeR\n",
    "latent_dims_z = [4, 8, 16] # for CSVAE and DISCoVeR\n",
    "latent_dims_w = [2, 4, 8] # for CSVAE and DISCoVeR\n",
    "encoder_hidden_dims_z = [1024, 512, 256, 128] # for CSVAE and DISCoVeR\n",
    "encoder_hidden_dims_w = [1024, 512, 256, 128] # for CSVAE and DISCoVeR\n",
    "adversarial_hidden_dims = [8] \n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "max_epochs = 5\n",
    "betas = (0.5, 1, 1.5, 2, 3, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fef2fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BaseVAE model...\n",
      "Training with beta=1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdamianrelkins\u001b[0m (\u001b[33mdamianrelkins-university-college-london-ucl-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250906_173346-s5kfhlqn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50/runs/s5kfhlqn' target=\"_blank\">pretty-gorge-10</a></strong> to <a href='https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50' target=\"_blank\">https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50/runs/s5kfhlqn' target=\"_blank\">https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50/runs/s5kfhlqn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type    | Params | Mode \n",
      "------------------------------------------\n",
      "0 | model | BaseVAE | 3.9 M  | train\n",
      "------------------------------------------\n",
      "3.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.9 M     Total params\n",
      "15.622    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 28538/28538 [05:04<00:00, 93.77it/s, v_num=hlqn, val_loss=147.0, val_bce=137.0, val_kld=10.20] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 28538/28538 [05:04<00:00, 93.77it/s, v_num=hlqn, val_loss=147.0, val_bce=137.0, val_kld=10.20]\n",
      "Testing DataLoader 0: 100%|██████████| 3568/3568 [00:14<00:00, 242.44it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_bce            136.44338989257812\n",
      "        test_kld            10.158361434936523\n",
      "        test_loss           146.60186767578125\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▅▅▅▅▅▅▆▆▆▆▆▆▆▆█████</td></tr><tr><td>test_bce</td><td>▁</td></tr><tr><td>test_kld</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_bce</td><td>█▅▄▃▄▃▄▂▃▃▁▁▂▂▂▂▃▁▂▃▂▁▁▂▂▂▂▂▃▂▁▂▁▃▁▂▂▁▁▁</td></tr><tr><td>train_kld</td><td>▁▂▄▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇▇▇██▇▇█▇██████████</td></tr><tr><td>train_loss</td><td>█▆▆▅▄▃▃▄▃▂▂▄▂▂▄▂▃▁▃▁▁▃▃▃▃▂▃▂▃▂▂▂▃▂▃▄▃▃▁▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>val_bce</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>val_kld</td><td>▁▄▅▆▇▇█▇██</td></tr><tr><td>+1</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>test_bce</td><td>136.44339</td></tr><tr><td>test_kld</td><td>10.15836</td></tr><tr><td>test_loss</td><td>146.60187</td></tr><tr><td>train_bce</td><td>138.96391</td></tr><tr><td>train_kld</td><td>10.32231</td></tr><tr><td>train_loss</td><td>149.28622</td></tr><tr><td>trainer/global_step</td><td>142690</td></tr><tr><td>val_bce</td><td>136.53821</td></tr><tr><td>val_kld</td><td>10.16389</td></tr><tr><td>+1</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pretty-gorge-10</strong> at: <a href='https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50/runs/s5kfhlqn' target=\"_blank\">https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50/runs/s5kfhlqn</a><br> View project at: <a href='https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50' target=\"_blank\">https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250906_173346-s5kfhlqn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BaseVAE model...\n",
      "Training with beta=1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.3s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250906_180536-astvolts</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50/runs/astvolts' target=\"_blank\">vague-voice-11</a></strong> to <a href='https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50' target=\"_blank\">https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50/runs/astvolts' target=\"_blank\">https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50/runs/astvolts</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type    | Params | Mode \n",
      "------------------------------------------\n",
      "0 | model | BaseVAE | 3.9 M  | train\n",
      "------------------------------------------\n",
      "3.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.9 M     Total params\n",
      "15.628    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 28538/28538 [04:42<00:00, 100.96it/s, v_num=olts, val_loss=147.0, val_bce=136.0, val_kld=10.10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 28538/28538 [04:42<00:00, 100.95it/s, v_num=olts, val_loss=147.0, val_bce=136.0, val_kld=10.10]\n",
      "Testing DataLoader 0: 100%|██████████| 3568/3568 [00:12<00:00, 285.29it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_bce            136.28671264648438\n",
      "        test_kld            10.088326454162598\n",
      "        test_loss           146.37486267089844\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▃▃▃▃▃▃▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆███████</td></tr><tr><td>test_bce</td><td>▁</td></tr><tr><td>test_kld</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_bce</td><td>█▅▂▄▅▄▃▃▃▃▂▃▃▃▄▂▄▃▃▃▃▂▃▃▃▃▃▃▂▃▃▃▃▁▃▃▂▂▂▄</td></tr><tr><td>train_kld</td><td>▁▃▄▅▅▅▆▆▆▇▇▇▆▆▇▇▇▇██▇▇▇██▇▇████▇███▇███▇</td></tr><tr><td>train_loss</td><td>▇▆▇█▇▇▆▄▁▄▇▃▄▁▆▄▄▄▄▅▃▄▂▅▆▅▄▃▂▃▅▄▆▄▂▁▄▃▅▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>val_bce</td><td>█▅▄▃▃▂▂▂▁▁</td></tr><tr><td>val_kld</td><td>▁▃▅▆▆▆█▇▇█</td></tr><tr><td>+1</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>test_bce</td><td>136.28671</td></tr><tr><td>test_kld</td><td>10.08833</td></tr><tr><td>test_loss</td><td>146.37486</td></tr><tr><td>train_bce</td><td>133.21512</td></tr><tr><td>train_kld</td><td>10.0992</td></tr><tr><td>train_loss</td><td>143.31432</td></tr><tr><td>trainer/global_step</td><td>142690</td></tr><tr><td>val_bce</td><td>136.4131</td></tr><tr><td>val_kld</td><td>10.09376</td></tr><tr><td>+1</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vague-voice-11</strong> at: <a href='https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50/runs/astvolts' target=\"_blank\">https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50/runs/astvolts</a><br> View project at: <a href='https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50' target=\"_blank\">https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250906_180536-astvolts/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BaseVAE model...\n",
      "Training with beta=1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250906_183221-q8i29y5o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50/runs/q8i29y5o' target=\"_blank\">decent-puddle-12</a></strong> to <a href='https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50' target=\"_blank\">https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50/runs/q8i29y5o' target=\"_blank\">https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50/runs/q8i29y5o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type    | Params | Mode \n",
      "------------------------------------------\n",
      "0 | model | BaseVAE | 3.9 M  | train\n",
      "------------------------------------------\n",
      "3.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.9 M     Total params\n",
      "15.641    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 28538/28538 [04:47<00:00, 99.16it/s, v_num=9y5o, val_loss=143.0, val_bce=131.0, val_kld=11.90] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 28538/28538 [04:47<00:00, 99.16it/s, v_num=9y5o, val_loss=143.0, val_bce=131.0, val_kld=11.90]\n",
      "Testing DataLoader 0: 100%|██████████| 3568/3568 [00:12<00:00, 284.84it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_bce            131.33856201171875\n",
      "        test_kld            11.871740341186523\n",
      "        test_loss           143.21054077148438\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▃▃▃▃▃▃▃▃▃▃▃▃▃▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆█████</td></tr><tr><td>test_bce</td><td>▁</td></tr><tr><td>test_kld</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_bce</td><td>█▇▇▆▅▆▄▅▆▄▅▄▅▄▄▄▃▃▅▃▃▄▂▃▁▄▂▃▂▄▃▁▂▄▂▂▁▂▂▃</td></tr><tr><td>train_kld</td><td>▁▃▄▅▆▆▆▇▆▇▇▇▇▇▇▇▇███████▇▇███▇██████████</td></tr><tr><td>train_loss</td><td>█▇▆▅▅▂▃▂▃▃▄▃▄▃▄▂▂▂▄▃▃▄▄▄▂▃▁▃▂▃▃▁▁▄▁▂▁▁▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>val_bce</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>val_kld</td><td>▁▄▄▆▆▆▆▇▇█</td></tr><tr><td>+1</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>test_bce</td><td>131.33856</td></tr><tr><td>test_kld</td><td>11.87174</td></tr><tr><td>test_loss</td><td>143.21054</td></tr><tr><td>train_bce</td><td>131.18587</td></tr><tr><td>train_kld</td><td>11.60181</td></tr><tr><td>train_loss</td><td>142.78767</td></tr><tr><td>trainer/global_step</td><td>142690</td></tr><tr><td>val_bce</td><td>131.41809</td></tr><tr><td>val_kld</td><td>11.87794</td></tr><tr><td>+1</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">decent-puddle-12</strong> at: <a href='https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50/runs/q8i29y5o' target=\"_blank\">https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50/runs/q8i29y5o</a><br> View project at: <a href='https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50' target=\"_blank\">https://wandb.ai/damianrelkins-university-college-london-ucl-/fg_vae_50</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250906_183221-q8i29y5o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for latent_dim, latent_dim_z, latent_dim_w in zip(latent_dims, latent_dims_z, latent_dims_w):\n",
    "    for MODEL in TRAIN:\n",
    "        model_type, dataset_type = MODEL.split('_')\n",
    "\n",
    "        if dataset_type == 'CUR':\n",
    "            # Load the curated dataset\n",
    "            curated_dataset = pd.read_pickle('data/chembl_35_fg_scaf_curated.pkl')\n",
    "\n",
    "            # Convert the fingerprint to numpy arrays\n",
    "            curated_dataset['fingerprint_array'] = curated_dataset['fingerprint_array'].apply(lambda x: x if isinstance(x, np.ndarray) else np.zeros((2048,), dtype=int))\n",
    "            curated_dataset['fg_array'] = curated_dataset['fg_array'].apply(lambda x: x if isinstance(x, np.ndarray) else np.zeros((100,), dtype=int))\n",
    "\n",
    "            MODEL_OUTPUT = 'models/small_models'  # Directory to save trained models\n",
    "            dataset = curated_dataset  # Use the curated dataset for training\n",
    "            sparse = False\n",
    "\n",
    "            input_dim = 2048\n",
    "            fg_dim = 4\n",
    "\n",
    "        elif dataset_type == 'FULL':\n",
    "            # Load the full dataset\n",
    "            full_dataset = pd.read_csv(\"data/chembl_35_fg_full.csv\")\n",
    "\n",
    "            MODEL_OUTPUT = 'models/large_models'  # Directory to save trained models\n",
    "            dataset = full_dataset  # Use the full dataset for training\n",
    "            sparse = True\n",
    "\n",
    "            input_dim = 2048\n",
    "            fg_dim = 50\n",
    "\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        if model_type is None:\n",
    "            raise ValueError(\"MODEL must be defined before training.\")\n",
    "        elif model_type == 'Base':\n",
    "\n",
    "            print(\"Training BaseVAE model...\")\n",
    "            \n",
    "            print(\"Training with beta=1 ...\")\n",
    "            vae_trainer = train_base_model(\n",
    "                dataset=dataset,\n",
    "                input_dim=input_dim,\n",
    "                latent_dim=latent_dim,\n",
    "                fg_dim=fg_dim,  # BaseVAE does not use fg_array for logging purposes only\n",
    "                encoder_hidden_dims=encoder_hidden_dims,\n",
    "                decoder_hidden_dims=decoder_hidden_dims,\n",
    "                beta=1,\n",
    "                batch_size=batch_size,\n",
    "                learning_rate=learning_rate,\n",
    "                max_epochs=max_epochs,\n",
    "                sparse=sparse\n",
    "            )\n",
    "\n",
    "        elif model_type == 'CVAE':\n",
    "\n",
    "            print(\"Training CVAE model...\")\n",
    "\n",
    "            vae_trainer = train_conditional_vae(\n",
    "                dataset=dataset,\n",
    "                fingerprint_dim=input_dim,\n",
    "                fg_dim=fg_dim,\n",
    "                latent_dim=latent_dim,\n",
    "                encoder_hidden_dims=encoder_hidden_dims,\n",
    "                decoder_hidden_dims=decoder_hidden_dims,\n",
    "                batch_size=batch_size,\n",
    "                learning_rate=learning_rate,\n",
    "                max_epochs=max_epochs,\n",
    "                sparse=sparse\n",
    "            )\n",
    "\n",
    "        elif model_type == 'CSVAE':\n",
    "\n",
    "            print(\"Training CSVAE model based on the NeurIPS 2018 paper...\")\n",
    "\n",
    "\n",
    "            vae_trainer = train_conditional_subspace_vae(\n",
    "                dataset=dataset,\n",
    "                fingerprint_dim=input_dim,\n",
    "                fg_dim=fg_dim,\n",
    "                latent_dim_z=latent_dim_z,\n",
    "                latent_dim_w=latent_dim_w,\n",
    "                encoder_hidden_dims_z=encoder_hidden_dims_z,\n",
    "                encoder_hidden_dims_w=encoder_hidden_dims_w,\n",
    "                decoder_hidden_dims=decoder_hidden_dims,\n",
    "                adversarial_hidden_dims=adversarial_hidden_dims,\n",
    "                batch_size=batch_size,\n",
    "                learning_rate=learning_rate,\n",
    "                max_epochs=max_epochs,\n",
    "                sparse=sparse\n",
    "            )\n",
    "\n",
    "        elif model_type == 'DISCoVeR':\n",
    "\n",
    "            print(\"Training DISCoVeR VAE model...\")\n",
    "\n",
    "            vae_trainer = train_discover_vae(\n",
    "                dataset=dataset,\n",
    "                fingerprint_dim=input_dim,\n",
    "                fg_dim=fg_dim,\n",
    "                latent_dim_z=latent_dim_z,\n",
    "                latent_dim_w=latent_dim_w,\n",
    "                encoder_hidden_dims_z=encoder_hidden_dims_z,\n",
    "                encoder_hidden_dims_w=encoder_hidden_dims_w,\n",
    "                decoder_hidden_dims=decoder_hidden_dims,\n",
    "                decoder_z_hidden_dims=decoder_z_hidden_dims,\n",
    "                adversarial_hidden_dims=adversarial_hidden_dims,\n",
    "                batch_size=batch_size,\n",
    "                learning_rate=learning_rate,\n",
    "                max_epochs=max_epochs,\n",
    "                sparse=sparse\n",
    "            )\n",
    "\n",
    "        # Free memory after each model\n",
    "        del vae_trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c590c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GET_LATENTS:\n",
    "    # Create a test DataLoader\n",
    "    curated_dataset = pd.read_pickle('data/chembl_35_fg_scaf_curated.pkl')\n",
    "\n",
    "    # Convert the fingerprint to numpy arrays\n",
    "    curated_dataset['fingerprint_array'] = curated_dataset['fingerprint_array'].apply(lambda x: x if isinstance(x, np.ndarray) else np.zeros((2048,), dtype=int))\n",
    "    curated_dataset['fg_array'] = curated_dataset['fg_array'].apply(lambda x: x if isinstance(x, np.ndarray) else np.zeros((100,), dtype=int))\n",
    "\n",
    "    # Split the dataset into train and test sets\n",
    "    train_data, test_data = train_test_split(curated_dataset, test_size=0.2, random_state=42)\n",
    "    val_data, test_data = train_test_split(test_data, test_size=0.2, random_state=42)\n",
    "\n",
    "    test_dataset = FingerprintDataset(test_data, sparse=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    train_dataset = FingerprintDataset(train_data, sparse=False)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "    dataset_partition = 'Test'\n",
    "\n",
    "    for latent_dim in latent_dims:\n",
    "        small_models = [\n",
    "            (\"Base\", f\"checkpoints/fg_bvae_4_{latent_dim}_1/best-checkpoint.ckpt\", BaseVAETrainer),\n",
    "            (\"CVAE\", f\"checkpoints/fg_cvae_4_{latent_dim}/best-checkpoint.ckpt\", ConditionalVAETrainer),\n",
    "            (\"CSVAE\", f\"checkpoints/fg_csvae_4_{latent_dim}/best-checkpoint.ckpt\", ConditionalSubspaceVAETrainer),\n",
    "            (\"DISCoVeR\", f\"checkpoints/fg_dvae_4_{latent_dim}/best-checkpoint.ckpt\", DiscoverVAETrainer),\n",
    "        ]\n",
    "\n",
    "        # Extract and save latents for small models\n",
    "        for model_name, model_path, model_class in small_models:\n",
    "            extract_and_save_latents(\n",
    "                model_path=model_path,\n",
    "                dataloader=test_dataloader if dataset_partition == 'Test' else train_dataloader,\n",
    "                model_type=model_name,\n",
    "                model_class=model_class,\n",
    "                device=device,\n",
    "                output_csv=f\"latents/latents_{model_name}_{len(test_dataset) if dataset_partition == 'Test' else len(train_dataset)}_{latent_dim}_{beta}.csv\"\n",
    "            ), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97679f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GET_EVAL:\n",
    "    # Create a test DataLoader\n",
    "    curated_dataset = pd.read_pickle('data/chembl_35_fg_scaf_curated.pkl')\n",
    "\n",
    "    # Convert the fingerprint to numpy arrays\n",
    "    curated_dataset['fingerprint_array'] = curated_dataset['fingerprint_array'].apply(lambda x: x if isinstance(x, np.ndarray) else np.zeros((2048,), dtype=int))\n",
    "    curated_dataset['fg_array'] = curated_dataset['fg_array'].apply(lambda x: x if isinstance(x, np.ndarray) else np.zeros((100,), dtype=int))\n",
    "\n",
    "    # Split the dataset into train and test sets\n",
    "    train_data, test_data = train_test_split(curated_dataset, test_size=0.2, random_state=42)\n",
    "    val_data, test_data = train_test_split(test_data, test_size=0.2, random_state=42)\n",
    "\n",
    "    test_dataset = FingerprintDataset(test_data, sparse=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Create list of models and locations\n",
    "    for beta in betas:\n",
    "        for latent_dim in latent_dims:\n",
    "            small_models = [\n",
    "                (\"Base\", f\"checkpoints/fg_bvae_4_{latent_dim}_{beta}/best-checkpoint.ckpt\", BaseVAETrainer),\n",
    "                # (\"CVAE\", f\"checkpoints/fg_cvae_4_{latent_dim}/best-checkpoint.ckpt\", ConditionalVAETrainer),\n",
    "                # (\"CSVAE\", f\"checkpoints/fg_csvae_4_{latent_dim}/best-checkpoint.ckpt\", ConditionalSubspaceVAETrainer),\n",
    "                # (\"DISCoVeR\", f\"checkpoints/fg_dvae_4_{latent_dim}/best-checkpoint.ckpt\", DiscoverVAETrainer)\n",
    "            ]\n",
    "\n",
    "            # Extract and save latents for small models\n",
    "            for model_name, model_path, model_class in small_models:\n",
    "                evaluate_reconstructions(\n",
    "                    model_path=model_path,\n",
    "                    dataloader=test_dataloader,\n",
    "                    model_type=model_name,\n",
    "                    model_class=model_class,\n",
    "                    device=device,\n",
    "                    thresholds=np.arange(0.4, 0.7, 0.02)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "774769b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Base model from checkpoints/fg_bvae_50_4_1/best-checkpoint.ckpt...\n",
      "Saved latents (including y and smiles) to latents/latents_Base_91321_4.csv\n",
      "Loading Base model from checkpoints/fg_bvae_50_8_1/best-checkpoint.ckpt...\n",
      "Saved latents (including y and smiles) to latents/latents_Base_91321_8.csv\n",
      "Loading Base model from checkpoints/fg_bvae_50_16_1/best-checkpoint.ckpt...\n",
      "Saved latents (including y and smiles) to latents/latents_Base_91321_16.csv\n"
     ]
    }
   ],
   "source": [
    "latent_dims = [4, 8, 16]  # You can change this to test different latent dimensions\n",
    "if GET_LATENTS:    \n",
    "    try:\n",
    "        # Set partition to either 'test' or 'train'\n",
    "        partition = 'test'  # or 'train'\n",
    "\n",
    "        full_dataset = pd.read_csv(\"data/chembl_35_fg_full.csv\")\n",
    "        train_full, test_full = train_test_split(full_dataset, test_size=0.2, random_state=42)\n",
    "        val_full, test_full = train_test_split(test_full, test_size=0.2, random_state=42)\n",
    "\n",
    "        if partition == 'test':\n",
    "            data_full = test_full\n",
    "        elif partition == 'train':\n",
    "            data_full = train_full\n",
    "        else:\n",
    "            raise ValueError(\"partition must be 'test' or 'train'\")\n",
    "\n",
    "        smiles_list = data_full['smiles'].tolist()\n",
    "        full_dataset_obj = FingerprintDataset(data_full, sparse=True)\n",
    "        full_dataloader = DataLoader(full_dataset_obj, batch_size=64, shuffle=False)\n",
    "\n",
    "        for latent_dim in latent_dims:\n",
    "            large_models = [\n",
    "                (\"Base\", f\"checkpoints/fg_bvae_50_{latent_dim}_1/best-checkpoint.ckpt\", BaseVAETrainer),\n",
    "                # (\"CVAE\", f\"checkpoints/fg_cvae_50_{latent_dim}/best-checkpoint.ckpt\", ConditionalVAETrainer),\n",
    "                # (\"CSVAE\", f\"checkpoints/fg_csvae_50_{latent_dim}/best-checkpoint.ckpt\", ConditionalSubspaceVAETrainer),\n",
    "                # (\"DISCoVeR\", f\"checkpoints/fg_dvae_50_{latent_dim}/best-checkpoint.ckpt\", DiscoverVAETrainer)\n",
    "            ]\n",
    "\n",
    "            for model_name, model_path, model_class in large_models:\n",
    "                extract_and_save_latents(\n",
    "                    model_path=model_path,\n",
    "                    dataloader=full_dataloader,\n",
    "                    model_type=model_name,\n",
    "                    model_class=model_class,\n",
    "                    device=\"mps\",\n",
    "                    smiles_list=smiles_list,\n",
    "                    output_csv=f\"latents/latents_{model_name}_{len(data_full)}_{latent_dim}.csv\"\n",
    "                )\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while extracting latents: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a88d8d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Base model from checkpoints/fg_bvae_50_8_1/best-checkpoint.ckpt...\n",
      "Selected threshold based on max F1: 0.640 (F1=0.5103)\n",
      "Saved reconstruction metrics to metrics/reconstruction_metrics_fg_bvae_50_8_1.csv\n",
      "Loading Base model from checkpoints/fg_bvae_50_16_1/best-checkpoint.ckpt...\n",
      "Selected threshold based on max F1: 0.640 (F1=0.5343)\n",
      "Saved reconstruction metrics to metrics/reconstruction_metrics_fg_bvae_50_16_1.csv\n"
     ]
    }
   ],
   "source": [
    "latent_dims = [8,16]  # You can change this to test different latent dimensions\n",
    "if GET_EVAL:\n",
    "    full_dataset = pd.read_csv(\"data/chembl_35_fg_full.csv\")\n",
    "\n",
    "    train_full, test_full = train_test_split(full_dataset, test_size=0.2, random_state=42)\n",
    "    val_full, test_full = train_test_split(test_full, test_size=0.2, random_state=42)\n",
    "\n",
    "    test_full_dataset = FingerprintDataset(test_full, sparse=True)\n",
    "    full_dataloader = DataLoader(test_full_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    for latent_dim in latent_dims:\n",
    "        large_models = [\n",
    "            (\"Base\", f\"checkpoints/fg_bvae_50_{latent_dim}_1/best-checkpoint.ckpt\", BaseVAETrainer),\n",
    "            # (\"CVAE\", f\"checkpoints/fg_cvae_50_{latent_dim}/best-checkpoint.ckpt\", ConditionalVAETrainer),\n",
    "            # (\"CSVAE\", f\"checkpoints/fg_csvae_50_{latent_dim}/best-checkpoint.ckpt\", ConditionalSubspaceVAETrainer),\n",
    "            # (\"DISCoVeR\", f\"checkpoints/fg_dvae_50_{latent_dim}/best-checkpoint.ckpt\", DiscoverVAETrainer)\n",
    "        ]\n",
    "\n",
    "        for model_name, model_path, model_class in large_models:\n",
    "            evaluate_reconstructions(\n",
    "                model_path=model_path,\n",
    "                dataloader=full_dataloader,\n",
    "                model_type=model_name,\n",
    "                model_class=model_class,\n",
    "                device=\"mps\",\n",
    "                thresholds=np.arange(0.5, 0.7, 0.02)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0138c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = {}\n",
    "models = ['Base', 'CSVAE', 'DISCoVeR']\n",
    "sizes = ['1826415']\n",
    "dims = ['4']\n",
    "neighbors = 50\n",
    "\n",
    "for model in models:\n",
    "    if model in ['Base', 'CVAE']:\n",
    "        terms = ['z', 'y']\n",
    "    elif model in ['CSVAE', 'DISCoVeR']:\n",
    "        terms = ['z', 'w', 'y']\n",
    "    for size in sizes:\n",
    "        for dim in dims:\n",
    "            name = f\"latents_{model}_{size}_{dim}\"\n",
    "            latent = f\"latents/{name}.csv\"\n",
    "            latents[name] = extract_prefixed_arrays(latent, terms)\n",
    "            if 'w' in terms:\n",
    "                # Concatenate w and z\n",
    "                latents[name]['wz'] = latents[name].apply(\n",
    "                    lambda row: np.concatenate([row['w'], row['z']]), axis=1\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7be03301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['latents_Base_1826415_4', 'latents_CSVAE_1826415_4', 'latents_DISCoVeR_1826415_4'])\n",
      "                                              smiles  \\\n",
      "0  Cc1c[nH]c2ncnc(N3CC4CC3CN4/C(=N/C#N)Nc3cccc(Br...   \n",
      "1                NC(C(=O)NO)C(=O)N1CCN(Cc2ccccc2)CC1   \n",
      "2  Cc1c(-c2noc(C(=O)N3CCC(Cc4ccccc4)CC3)n2)oc2c(C...   \n",
      "3  COc1cc(C(=O)O)ccc1NC(=O)[C@H]1[C@H](c2cccc(Cl)...   \n",
      "4         Cn1c([C@H]2CCN(Cc3ccc(Cl)cc3)C2)nc2ccccc21   \n",
      "\n",
      "                                                  z  \\\n",
      "0   [0.36363792, -1.1114069, 0.57658255, 1.0574797]   \n",
      "1    [1.7424464, 0.95360804, 0.5800475, 0.35313958]   \n",
      "2  [0.9032097, -0.015957396, 0.3586692, 0.18548699]   \n",
      "3  [0.20626788, 0.1730261, -1.5167452, -0.12495865]   \n",
      "4  [0.62835634, 1.3160388, 0.020977855, 0.31995744]   \n",
      "\n",
      "                                                   y  \n",
      "0  [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "1  [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "2  [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "3  [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
      "4  [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, ...  \n"
     ]
    }
   ],
   "source": [
    "print(latents.keys())\n",
    "print(latents['latents_Base_1826415_4'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28cb62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'umap'\n",
    "cur_size = 80000\n",
    "full_size = 1826415\n",
    "latent_dim = 4\n",
    "sample_size = 100000\n",
    "beta = 1\n",
    "fg_names = ['[R][NH][R]', 'O=[C](O)[R]', 'C=C', '[NH2][Car]']\n",
    "\n",
    "for latent_dim in latent_dims:\n",
    "    if VIS_CUR:\n",
    "        try:\n",
    "            # Visualize the latent space\n",
    "            visualize_latent_space_per_fg(latents[f'latents_Base_{cur_size}_{latent_dim}_{beta}']['z'], latents[f'latents_Base_{cur_size}_{latent_dim}_{beta}']['y'], method, sample_size=sample_size, combined_title=f'Base VAE (Beta = {beta}) {method.upper()} {latent_dim} Dimensions {'Test Set' if cur_size < 50000 else 'Train Set'}', per_fg_title=f'Base VAE (Beta = {beta}) by FG {method.upper()} {latent_dim} {'Test Set' if cur_size < 50000 else 'Train Set'}', save_path=f'images/base_vae_cur_{latent_dim}_{cur_size}_{beta}/', fg_names=fg_names)\n",
    "            visualize_latent_space_per_fg(latents[f'latents_CVAE_{cur_size}_{latent_dim}']['z'], latents[f'latents_CVAE_{cur_size}_{latent_dim}']['y'], method, sample_size=sample_size, combined_title=f'CVAE {method.upper()} {latent_dim} {'Test Set' if cur_size < 50000 else 'Train Set'}', per_fg_title=f'CVAE by FG {method.upper()} {latent_dim} {'Test Set' if cur_size < 50000 else 'Train Set'}', save_path=f'images/cvae_cur_{latent_dim}_{cur_size}/', fg_names=fg_names)\n",
    "            visualize_latent_space_per_fg(latents[f'latents_CSVAE_{cur_size}_{latent_dim}']['z'], latents[f'latents_CSVAE_{cur_size}_{latent_dim}']['y'], method, sample_size=sample_size, combined_title=f'CSVAE {method.upper()} {latent_dim} {'Test Set' if cur_size < 50000 else 'Train Set'}', per_fg_title=f'CSVAE by FG {method.upper()} {latent_dim} {'Test Set' if cur_size < 50000 else 'Train Set'}', save_path=f'images/csvae_cur_{latent_dim}_{cur_size}/', fg_names=fg_names)\n",
    "            visualize_latent_space_per_fg(latents[f'latents_DISCoVeR_{cur_size}_{latent_dim}']['z'], latents[f'latents_DISCoVeR_{cur_size}_{latent_dim}']['y'], method, sample_size=sample_size, combined_title=f'Discover {method.upper()} {latent_dim} {'Test Set' if cur_size < 50000 else 'Train Set'}', per_fg_title=f'Discover by FG {method.upper()} {latent_dim} {'Test Set' if cur_size < 50000 else 'Train Set'}', save_path=f'images/discover_cur_{latent_dim}_{cur_size}/', fg_names=fg_names)\n",
    "            visualize_latent_space_per_fg(latents[f'latents_CSVAE_{cur_size}_{latent_dim}']['w'], latents[f'latents_CSVAE_{cur_size}_{latent_dim}']['y'], method, sample_size=sample_size, combined_title=f'CSVAE {method.upper()} {int(latent_dim/2)} {'Test Set' if cur_size < 50000 else 'Train Set'}', per_fg_title=f'CSVAE by FG {method.upper()} {int(latent_dim/2)} {'Test Set' if cur_size < 50000 else 'Train Set'}', save_path=f'images/csvae_cur_w_{latent_dim}_{cur_size}/', fg_names=fg_names)\n",
    "            visualize_latent_space_per_fg(latents[f'latents_DISCoVeR_{cur_size}_{latent_dim}']['w'], latents[f'latents_DISCoVeR_{cur_size}_{latent_dim}']['y'], method, sample_size=sample_size, combined_title=f'Discover {method.upper()} {int(latent_dim/2)} {'Test Set' if cur_size < 50000 else 'Train Set'}', per_fg_title=f'Discover by FG {method.upper()} {int(latent_dim/2)} {'Test Set' if cur_size < 50000 else 'Train Set'}', save_path=f'images/discover_cur_w_{latent_dim}_{cur_size}/', fg_names=fg_names)\n",
    "            visualize_latent_space_per_fg(latents[f'latents_CSVAE_{cur_size}_{latent_dim}']['wz'], latents[f'latents_CSVAE_{cur_size}_{latent_dim}']['y'], method, sample_size=sample_size, combined_title=f'CSVAE {method.upper()} {latent_dim+int(latent_dim/2)} {'Test Set' if cur_size < 50000 else 'Train Set'}', per_fg_title=f'CSVAE WZ by FG {method.upper()} {latent_dim+int(latent_dim/2)} {'Test Set' if cur_size < 50000 else 'Train Set'}', save_path=f'images/csvae_cur_wz_{latent_dim}_{cur_size}/', fg_names=fg_names)\n",
    "            visualize_latent_space_per_fg(latents[f'latents_DISCoVeR_{cur_size}_{latent_dim}']['wz'], latents[f'latents_DISCoVeR_{cur_size}_{latent_dim}']['y'], method, sample_size=sample_size, combined_title=f'Discover {method.upper()} {latent_dim+int(latent_dim/2)} {'Test Set' if cur_size < 50000 else 'Train Set'}', per_fg_title=f'Discover WZ by FG {method.upper()} {latent_dim+int(latent_dim/2)} {'Test Set' if cur_size < 50000 else 'Train Set'}', save_path=f'images/discover_cur_wz_{latent_dim}_{cur_size}/', fg_names=fg_names)\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while visualizing current latents: {e}\")\n",
    "\n",
    "    if VIS_FULL:\n",
    "        try:\n",
    "            visualize_latent_space_per_fg(latents[f'latents_Base_{full_size}_50']['z'], latents[f'latents_Base_{full_size}_50']['y'], method, sample_size=5000, combined_title=f'Base VAE {method.upper()} {latent_dim}', per_fg_title=f'Base VAE by FG {method.upper()} {latent_dim}', save_path=f'images/base_vae_full_{latent_dim}_{full_size}/')\n",
    "            visualize_latent_space_per_fg(latents[f'latents_CVAE_{full_size}_50']['z'], latents[f'latents_CVAE_{full_size}_50']['y'], method, sample_size=5000, combined_title=f'CVAE {method.upper()} {latent_dim}', per_fg_title=f'CVAE by FG {method.upper()} {latent_dim}', save_path=f'images/cvae_full_{latent_dim}_{full_size}/')\n",
    "            visualize_latent_space_per_fg(latents[f'latents_CSVAE_{full_size}_50']['z'], latents[f'latents_CSVAE_{full_size}_50']['y'], method, sample_size=5000, combined_title=f'CSVAE {method.upper()} {latent_dim}', per_fg_title=f'CSVAE by FG {method.upper()} {latent_dim}', save_path=f'images/csvae_full_{latent_dim}_{full_size}/')\n",
    "            visualize_latent_space_per_fg(latents[f'latents_DISCoVeR_{full_size}_50']['z'], latents[f'latents_DISCoVeR_{full_size}_50']['y'], method, sample_size=5000, combined_title=f'Discover {method.upper()} {latent_dim}', per_fg_title=f'Discover by FG {method.upper()} {latent_dim}', save_path=f'images/discover_full_{latent_dim}_{full_size}/')\n",
    "            visualize_latent_space_per_fg(latents[f'latents_CSVAE_{full_size}_50']['w'], latents[f'latents_CSVAE_{full_size}_50']['y'], method, sample_size=5000, combined_title=f'CSVAE {method.upper()} {latent_dim}', per_fg_title=f'CSVAE by FG {method.upper()} {latent_dim}', save_path=f'images/csvae_full_w_{latent_dim}_{full_size}/')\n",
    "            visualize_latent_space_per_fg(latents[f'latents_DISCoVeR_{full_size}_50']['w'], latents[f'latents_DISCoVeR_{full_size}_50']['y'], method, sample_size=5000, combined_title=f'Discover {method.upper()} {latent_dim}', per_fg_title=f'Discover by FG {method.upper()} {latent_dim}', save_path=f'images/discover_full_w_{latent_dim}_{full_size}/')\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while visualizing full latents: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a9c03c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing latent sets: 100%|██████████| 6/6 [02:22<00:00, 23.83s/it]\n"
     ]
    }
   ],
   "source": [
    "# load all latents into latents list\n",
    "\n",
    "# Prepare to collect metrics\n",
    "metrics_rows = []\n",
    "header = ['model', 'size', 'dim', 'beta', 'latent_type', 'score']\n",
    "\n",
    "# Only outer progress bar\n",
    "for name, latent in tqdm(latents.items(), desc=\"Processing latent sets\", dynamic_ncols=True):\n",
    "    parts = name.split('_')\n",
    "    model, size, dim = parts[1], parts[2], parts[3]\n",
    "    fg_counts = latent['y'].sum()\n",
    "\n",
    "    latent_sample = latent.sample(n=50000) if len(latent['y']) > 50000 else latent\n",
    "\n",
    "    def process_latent(latent_key):\n",
    "        nbrs, latent_data, labels = get_nearest_neighbors(\n",
    "            latents=latent_sample[latent_key].to_list(),\n",
    "            fg_labels=np.array(latent_sample['y'].to_list()),\n",
    "            n_neighbors=neighbors\n",
    "        )\n",
    "        scores = [metric(nbrs, i, latent_data, fg_counts, labels, neighbors)\n",
    "                  for i in range(len(latent_sample['y']))]\n",
    "\n",
    "        mean_score = np.mean(scores)\n",
    "        return [model, size, dim, latent_key, mean_score]\n",
    "\n",
    "    metrics_rows.append(process_latent('z'))\n",
    "\n",
    "    if 'w' in latent_sample:\n",
    "        metrics_rows.append(process_latent('w'))\n",
    "        \n",
    "    if 'wz' in latent_sample:\n",
    "        metrics_rows.append(process_latent('wz'))\n",
    "\n",
    "# Save to CSV \n",
    "metrics_path = Path(f'metrics/latent_metrics_{neighbors}.csv')\n",
    "with open(metrics_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(metrics_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "70beb903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure top-level directories exist\n",
    "Path('images').mkdir(exist_ok=True)\n",
    "Path('interpolations').mkdir(exist_ok=True)\n",
    "\n",
    "# Compute descriptors\n",
    "def compute_descriptors(mol):\n",
    "    return {\n",
    "        'MolWt': Descriptors.MolWt(mol),\n",
    "        'LogP': Descriptors.MolLogP(mol),\n",
    "        'QED': QED.qed(mol),\n",
    "    }\n",
    "\n",
    "def save_molecule_image(mol, smi, output_dir, anchor_mol=None):\n",
    "    # Generate molecule image\n",
    "    # Generate molecule image\n",
    "    img_text, _, _, _ = get_dec_fgs(mol)\n",
    "    img = Image.open(io.BytesIO(img_text)).convert(\"RGB\")  # ensure RGB\n",
    "\n",
    "    # Compute descriptors\n",
    "    desc = compute_descriptors(mol)\n",
    "    desc_text = f\"MolWt: {desc['MolWt']:.2f}, LogP: {desc['LogP']:.2f}, QED: {desc['QED']:.2f}\"\n",
    "\n",
    "    # Compute Tanimoto similarity if anchor provided\n",
    "    if anchor_mol is not None:\n",
    "        fp_mol = mol_to_fingerprint(mol)\n",
    "        fp_anchor = mol_to_fingerprint(anchor_mol)\n",
    "        tanimoto = DataStructs.TanimotoSimilarity(fp_mol, fp_anchor)\n",
    "        desc_text += f\", Tanimoto: {tanimoto:.2f}\"\n",
    "        print(f\"Tanimoto similarity between {Chem.MolToSmiles(mol)} and anchor: {tanimoto:.2f}\")\n",
    "\n",
    "    # Draw text size and dynamically fit\n",
    "    draw_temp = ImageDraw.Draw(img)\n",
    "    max_width = img.width - 10\n",
    "    font_size = 48\n",
    "    while font_size > 8:\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"/System/Library/Fonts/Supplemental/Arial.ttf\", size=font_size)\n",
    "        except OSError:\n",
    "            font = ImageFont.load_default()\n",
    "        bbox = draw_temp.textbbox((0,0), desc_text, font=font)\n",
    "        text_width = bbox[2] - bbox[0]\n",
    "        text_height = bbox[3] - bbox[1]\n",
    "        if text_width <= max_width:\n",
    "            break\n",
    "        font_size -= 2\n",
    "\n",
    "    # Create new image\n",
    "    total_height = img.height + text_height + 10\n",
    "    new_img = Image.new(\"RGB\", (img.width, total_height), \"white\")\n",
    "    new_img.paste(img, (0, 0))\n",
    "\n",
    "    draw = ImageDraw.Draw(new_img)\n",
    "    draw.text(((img.width - text_width)//2, img.height + 5), desc_text, fill=\"black\", font=font)\n",
    "\n",
    "    # Save image\n",
    "    safe_smi = re.sub(r'[^\\w\\-]', '_', smi)\n",
    "    file_path = os.path.join(output_dir, f\"{safe_smi}.png\")\n",
    "    new_img.save(file_path)\n",
    "\n",
    "# SLERP interpolation\n",
    "def slerp(z1, z2, t):\n",
    "    z1 = np.array(z1)\n",
    "    z2 = np.array(z2)\n",
    "    omega = np.arccos(np.clip(np.dot(z1, z2) / (np.linalg.norm(z1) * np.linalg.norm(z2)), -1, 1))\n",
    "    so = np.sin(omega)\n",
    "    if so == 0:\n",
    "        return (1 - t) * z1 + t * z2\n",
    "    return (np.sin((1 - t) * omega) / so) * z1 + (np.sin(t * omega) / so) * z2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d829c8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using anchor index: 1051396\n",
      "Visualizing nearest neighbors for Base with latent dim 4 and dataset size 1826415\n",
      "Tanimoto similarity between CC(C)Cc1ccc(C(C)C(=O)O)cc1 and anchor: 1.00\n",
      "Tanimoto similarity between Oc1c(C(Nc2ccccc2)c2ccccc2)cc(Br)c2ccccc12 and anchor: 0.13\n",
      "Tanimoto similarity between CC(C)NC(C)C(O)c1cccc(Cl)c1 and anchor: 0.17\n",
      "Tanimoto similarity between CN(C)C(c1ccccc1)C(O)c1ccccc1 and anchor: 0.21\n",
      "Tanimoto similarity between CC(C)CC(N)(Cc1ccccc1)C(=O)O and anchor: 0.32\n",
      "Tanimoto similarity between CC(c1ccccc1)C(C(=O)O)C(Cc1ccccc1)C(=O)O and anchor: 0.45\n",
      "Anchor: CC(C)Cc1ccc(C(C)C(=O)O)cc1\n",
      "Nearest neighbors: ['Oc1c(C(Nc2ccccc2)c2ccccc2)cc(Br)c2ccccc12', 'CC(C)NC(C)C(O)c1cccc(Cl)c1', 'CN(C)C(c1ccccc1)C(O)c1ccccc1', 'CC(C)C[C@](N)(Cc1ccccc1)C(=O)O', 'C[C@@H](c1ccccc1)[C@H](C(=O)O)[C@H](Cc1ccccc1)C(=O)O']\n",
      "Visualizing nearest neighbors for CSVAE with latent dim 4 and dataset size 1826415\n",
      "Tanimoto similarity between CC(C)Cc1ccc(C(C)C(=O)O)cc1 and anchor: 1.00\n",
      "Tanimoto similarity between NC(C(=O)O)c1ccc(C(=O)O)c(C(=O)O)c1 and anchor: 0.28\n",
      "Tanimoto similarity between NC(=O)C(O)c1ccccc1 and anchor: 0.25\n",
      "Tanimoto similarity between O=C(O)C(CCC(C(=O)O)c1ccccc1)c1ccccc1 and anchor: 0.32\n",
      "Tanimoto similarity between O=P(O)(O)C(Cc1ccc(-c2ccccc2)cc1)c1ccccc1 and anchor: 0.32\n",
      "Tanimoto similarity between CP(=O)(O)C(N)c1ccccc1 and anchor: 0.23\n",
      "Anchor: CC(C)Cc1ccc(C(C)C(=O)O)cc1\n",
      "Nearest neighbors: ['N[C@H](C(=O)O)c1ccc(C(=O)O)c(C(=O)O)c1', 'NC(=O)[C@H](O)c1ccccc1', 'O=C(O)C(CCC(C(=O)O)c1ccccc1)c1ccccc1', 'O=P(O)(O)C(Cc1ccc(-c2ccccc2)cc1)c1ccccc1', 'CP(=O)(O)C(N)c1ccccc1']\n",
      "Visualizing nearest neighbors for DISCoVeR with latent dim 4 and dataset size 1826415\n",
      "Tanimoto similarity between CC(C)Cc1ccc(C(C)C(=O)O)cc1 and anchor: 1.00\n",
      "Tanimoto similarity between CC(CO)Cc1ccc(C(C)C(=O)O)cc1 and anchor: 0.71\n",
      "Tanimoto similarity between NC(CSC(c1ccccc1)(c1ccccc1)c1ccc(O)cc1)C(=O)O and anchor: 0.21\n",
      "Tanimoto similarity between CC(C)Cc1ccc(CC(=O)O)cc1 and anchor: 0.60\n",
      "Tanimoto similarity between O=C(O)CC(CSCCS)Cc1ccccc1 and anchor: 0.26\n",
      "Tanimoto similarity between CCCCOC(=O)CCSC1=C(Sc2ccc(O)cc2)C(=O)c2ccccc2C1=O and anchor: 0.14\n",
      "Anchor: CC(C)Cc1ccc(C(C)C(=O)O)cc1\n",
      "Nearest neighbors: ['CC(CO)Cc1ccc(C(C)C(=O)O)cc1', 'N[C@@H](CSC(c1ccccc1)(c1ccccc1)c1ccc(O)cc1)C(=O)O', 'CC(C)Cc1ccc(CC(=O)O)cc1', 'O=C(O)CC(CSCCS)Cc1ccccc1', 'CCCCOC(=O)CCSC1=C(Sc2ccc(O)cc2)C(=O)c2ccccc2C1=O']\n"
     ]
    }
   ],
   "source": [
    "def visualize_nearest_neighbors(latent_df, latent_key='z', anchor_idx=None, output_root=\"images/nearest_neighbors\", n_neighbors=5):\n",
    "    \"\"\"\n",
    "    Selects a molecule (by anchor_idx if provided, else random) and visualizes its n nearest neighbors in latent space.\n",
    "    \n",
    "    latent_df: pandas DataFrame with columns ['smiles', 'z']\n",
    "    anchor_idx: index of anchor molecule (int), or None for random\n",
    "    output_root: root directory to save images\n",
    "    n_neighbors: number of neighbors to visualize\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    import numpy as np\n",
    "    import random\n",
    "    import re\n",
    "    from rdkit import Chem\n",
    "\n",
    "    Path(output_root).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Pick anchor molecule\n",
    "    if anchor_idx is None:\n",
    "        idx = random.randint(0, len(latent_df) - 1)\n",
    "    else:\n",
    "        idx = anchor_idx\n",
    "    anchor_smi = latent_df.iloc[idx]['smiles']\n",
    "    anchor_z = np.array(latent_df.iloc[idx][latent_key])\n",
    "\n",
    "    # Compute distances\n",
    "    latents_array = np.vstack(latent_df[latent_key].to_numpy())\n",
    "    distances = np.linalg.norm(latents_array - anchor_z, axis=1)\n",
    "    distances[idx] = np.inf  # exclude itself\n",
    "\n",
    "    # Find n nearest neighbors\n",
    "    neighbor_indices = np.argsort(distances)[:n_neighbors]\n",
    "    neighbor_smiles = [latent_df.iloc[i]['smiles'] for i in neighbor_indices]\n",
    "\n",
    "    # Create output folder for this anchor\n",
    "    safe_anchor = re.sub(r'[^\\w\\-]', '_', anchor_smi)\n",
    "    out_dir = Path(output_root) / safe_anchor\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save anchor as \"ibuprofen.png\"\n",
    "    anchor_mol = Chem.MolFromSmiles(anchor_smi)\n",
    "    if anchor_mol:\n",
    "        save_molecule_image(anchor_mol, \"ibuprofen\", output_dir=str(out_dir), anchor_mol=anchor_mol)\n",
    "\n",
    "    # Save neighbors as neighbour1.png ... neighbourN.png\n",
    "    for i, smi in enumerate(neighbor_smiles, 1):\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol:\n",
    "            save_molecule_image(mol, f\"neighbour{i}\", output_dir=str(out_dir), anchor_mol=anchor_mol)\n",
    "\n",
    "    print(f\"Anchor: {anchor_smi}\")\n",
    "    print(f\"Nearest neighbors: {neighbor_smiles}\")\n",
    "    return idx, anchor_smi, neighbor_smiles\n",
    "\n",
    "\n",
    "# Choose a fixed anchor index for all models\n",
    "anchor_idx = random.randint(0, len(next(iter(latents.values()))) - 1)\n",
    "anchor_idx = 1051396 # Ibuprofen\n",
    "print(f\"Using anchor index: {anchor_idx}\")\n",
    "\n",
    "# Visualize nearest neighbors for each model using the same anchor\n",
    "for name, latent_df in latents.items():\n",
    "    parts = name.split('_')\n",
    "    model, size, dim = parts[1], parts[2], parts[3]\n",
    "    print(f\"Visualizing nearest neighbors for {model} with latent dim {dim} and dataset size {size}\")\n",
    "    if 'Base' in model:\n",
    "        latent_key = 'z'\n",
    "    elif 'CSVAE' in model or 'DISCoVeR' in model:\n",
    "        latent_key = 'wz'\n",
    "    visualize_nearest_neighbors(latent_df, latent_key=latent_key, anchor_idx=anchor_idx, output_root=f\"images/nearest_neighbors/{model}_{size}_{dim}\", n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73790a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use metric directly on fingerprints\n",
    "fingerprint_metric_df = pd.read_pickle('data/chembl_35_fg_scaf_curated.pkl')\n",
    "\n",
    "fingerprint_train, fingerprint_test = train_test_split(fingerprint_metric_df, test_size=0.2, random_state=42)\n",
    "fingerprint_val, fingerprint_test = train_test_split(fingerprint_test, test_size=0.2, random_state=42)\n",
    "\n",
    "# select a subset of fingerprints for evaluation\n",
    "fingerprint_train = fingerprint_train.sample(n=10000, random_state=42)\n",
    "\n",
    "nbrs, fingerprints, labels = get_nearest_neighbors(latents=fingerprint_train['fingerprint_array'].to_list(), fg_labels=fingerprint_train['fg_array'].to_list(), n_neighbors=50)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for i in range(len(fingerprints)):\n",
    "    score = metric(nbrs, i, fingerprints, fingerprint_train['fg_array'].sum(), labels, 50)\n",
    "    scores.append(score)\n",
    "\n",
    "print(np.array(scores).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0803dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use metric directly on fingerprints\n",
    "fingerprint_metric_df = pd.read_csv('data/chembl_35_fg_full.csv')\n",
    "\n",
    "fingerprint_train, fingerprint_test = train_test_split(fingerprint_metric_df, test_size=0.2, random_state=42)\n",
    "fingerprint_val, fingerprint_test = train_test_split(fingerprint_test, test_size=0.2, random_state=42)\n",
    "\n",
    "# select a subset of fingerprints for evaluation\n",
    "fingerprint_train = fingerprint_train.sample(n=10000, random_state=42)\n",
    "\n",
    "fingerprint_train_dataset = FingerprintDataset(fingerprint_train, sparse=True)\n",
    "\n",
    "fingerprint_train_loader = DataLoader(fingerprint_train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Concatenate batches into a single tensor\n",
    "fingerprints = torch.cat([batch[0] for batch in fingerprint_train_loader], dim=0)\n",
    "fg_vectors = torch.cat([batch[1] for batch in fingerprint_train_loader], dim=0)\n",
    "\n",
    "# Convert to numpy if needed\n",
    "fingerprints = fingerprints.numpy()\n",
    "fg_vectors = fg_vectors.numpy()\n",
    "\n",
    "# Now call nearest neighbors\n",
    "nbrs, fingerprints, labels = get_nearest_neighbors(\n",
    "    latents=fingerprints, \n",
    "    fg_labels=fg_vectors, \n",
    "    n_neighbors=50\n",
    ")\n",
    "\n",
    "\n",
    "scores = []\n",
    "\n",
    "for i in range(len(fingerprints)):\n",
    "    score = metric(nbrs, i, fingerprints, fg_vectors.sum(), labels, 50)\n",
    "    scores.append(score)\n",
    "\n",
    "print(np.array(scores).mean())\n",
    "\n",
    "# Get tanimoto similarity matrix\n",
    "tanimoto_sim_matrix = np.zeros((len(fingerprints), len(fingerprints)))\n",
    "\n",
    "for i in range(len(fingerprints)):\n",
    "    for j in range(len(fingerprints)):\n",
    "        tanimoto_sim_matrix[i, j] = TanimotoSimilarity(fingerprint_to_bv(fingerprints[i]), fingerprint_to_bv(fingerprints[j]))\n",
    "\n",
    "# only keep upper triangle\n",
    "tanimoto_sim_matrix = np.triu(tanimoto_sim_matrix, k=1)\n",
    "\n",
    "# get descriptive statistics\n",
    "tanimoto_sim_flat = tanimoto_sim_matrix.flatten()\n",
    "tanimoto_sim_flat = tanimoto_sim_flat[tanimoto_sim_flat > 0]  # keep only positive similarities\n",
    "\n",
    "print(\"Tanimoto Similarity - Descriptive Statistics:\")\n",
    "print(f\"Mean: {tanimoto_sim_flat.mean()}\")\n",
    "print(f\"Median: {np.median(tanimoto_sim_flat)}\")\n",
    "print(f\"Std Dev: {tanimoto_sim_flat.std()}\")\n",
    "print(f\"Max: {tanimoto_sim_flat.max()}\")\n",
    "print(f\"Min: {tanimoto_sim_flat.min()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84201d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fingerprint brightness umap\n",
    "fingerprint_metric_df = pd.read_pickle('data/chembl_35_fg_scaf_curated.pkl')\n",
    "\n",
    "fingerprint_train, fingerprint_test = train_test_split(fingerprint_metric_df, test_size=0.2, random_state=42)\n",
    "fingerprint_val, fingerprint_test = train_test_split(fingerprint_test, test_size=0.2, random_state=42)\n",
    "\n",
    "fingerprint_train_dataset = FingerprintDataset(fingerprint_train, sparse=False)\n",
    "\n",
    "model = BaseVAETrainer.load_from_checkpoint('checkpoints/fg_bvae_4_8/best-checkpoint.ckpt', map_location='cpu').model\n",
    "\n",
    "visualize_latent_space_tanimoto(\n",
    "    model=model,\n",
    "    dataset=fingerprint_train_dataset,\n",
    "    method='umap',\n",
    "    sample_size=100000,\n",
    "    title='Base VAE Latent Space with Tanimoto Brightness Scale (UMAP)',\n",
    "    save_path='figures/fingerprint_latent_space_umap.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353e8b15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdkit-thesis-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
